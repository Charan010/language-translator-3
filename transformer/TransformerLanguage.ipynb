{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aNu15GFaB58C",
        "outputId": "2e9e848e-5739-43be-ca21-1b99b26cc0f0"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to\n",
            "[nltk_data]     C:\\Users\\Charan\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     C:\\Users\\Charan\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to\n",
            "[nltk_data]     C:\\Users\\Charan\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers\\punkt_tab.zip.\n"
          ]
        }
      ],
      "source": [
        "import unicodedata\n",
        "import pandas as pd\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download(\"punkt_tab\")\n",
        "import ast"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kc8Oo-34c5aN",
        "outputId": "6a4f13af-f3f4-479e-9232-72040c50208a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                                               English  \\\n",
            "0    Lionel Messi is the only player in history to ...   \n",
            "1    The Great Wall of China cannot be seen from sp...   \n",
            "2    My Wi-Fi always decides to stop working the mo...   \n",
            "3    Michael Phelps has won more Olympic gold medal...   \n",
            "4    Every time I go to the grocery store, I forget...   \n",
            "..                                                 ...   \n",
            "245  Beethoven composed some of his greatest works ...   \n",
            "246  The world’s most expensive coffee comes from b...   \n",
            "247  The human nose can remember over 50,000 differ...   \n",
            "248  The International Space Station orbits Earth a...   \n",
            "249  The first video ever uploaded to YouTube was a...   \n",
            "\n",
            "                                                Telugu  \n",
            "0    లియోనెల్ మెస్సీ ఎనిమిది బాలోన్ డి'ఓర్ అవార్డుల...  \n",
            "1    చైనా గ్రేట్ వాల్ ను అంతరిక్షం నుండి నగ్న కళ్ళత...  \n",
            "2    నాకు ఏదైనా ముఖ్యమైన పని ఉన్నప్పుడు నా వై-ఫై ఎల...  \n",
            "3    మైఖేల్ ఫెల్ప్స్ ఒలింపిక్ క్రీడల చరిత్రలో చాలా ...  \n",
            "4    నేను కిరాణా దుకాణానికి వెళ్లిన ప్రతిసారీ, నాకు...  \n",
            "..                                                 ...  \n",
            "245  బీథోవెన్ తన గొప్ప రచనలలో కొన్నింటిని పూర్తిగా ...  \n",
            "246  ప్రపంచంలో అత్యంత ఖరీదైన కాఫీ, సివెట్ పిల్లి జీ...  \n",
            "247  మానవ ముక్కు 50,000 విభిన్న వాసనలను గుర్తుంచుకో...  \n",
            "248  ఇంటర్నేషనల్ స్పేస్ స్టేషన్ గంటకు 28,000 కిలోమీ...  \n",
            "249  యూట్యూబ్కి అప్లోడ్ చేయబడిన మొదటి వీడియో ఒక జూల...  \n",
            "\n",
            "[250 rows x 2 columns]\n",
            "Normalization complete. Normalized data saved to: Normalized_Dataset.xlsx\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "data = \"/content/english_telugu_dataset_extra.xlsx\"\n",
        "df = pd.read_excel(data)\n",
        "print(df)\n",
        "df.columns = ['English_Text', 'Telugu_Text']\n",
        "\n",
        "def normalize_text(text):\n",
        "    return unicodedata.normalize(\"NFKC\", str(text))\n",
        "\n",
        "df['Normalized_ENGLISH_Text'] = df['English_Text'].apply(normalize_text)\n",#normalized text
        "df['Normalized_TELUGU_Text'] = df['Telugu_Text'].apply(normalize_text)\n",
        "\n",
        "normalized_df = df[['Normalized_ENGLISH_Text', 'Normalized_TELUGU_Text']]\n",
        "\n",
        "output_file = \"Normalized_Dataset.xlsx\"\n",
        "\n",
        "normalized_df.to_excel(output_file, index=False)\n",
        "\n",
        "print(\"Normalization complete. Normalized data saved to:\", output_file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "elawoon1gTTq",
        "outputId": "b9280746-ae42-4058-94e0-fae68c290254"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Normalized_ENGLISH_Text after lowercasing english text \n",
            "\n",
            "0      lionel messi is the only player in history to ...\n",
            "1      the great wall of china cannot be seen from sp...\n",
            "2      my wi-fi always decides to stop working the mo...\n",
            "3      michael phelps has won more olympic gold medal...\n",
            "4      every time i go to the grocery store, i forget...\n",
            "                             ...                        \n",
            "245    beethoven composed some of his greatest works ...\n",
            "246    the world’s most expensive coffee comes from b...\n",
            "247    the human nose can remember over 50,000 differ...\n",
            "248    the international space station orbits earth a...\n",
            "249    the first video ever uploaded to youtube was a...\n",
            "Name: Normalized_ENGLISH_Text, Length: 250, dtype: object\n",
            "\n",
            "\n",
            "Normalized_ENGLISH_Text and Normalized_Telugu_Text after removing punctuations  \n",
            "\n",
            "                                          English_Text  \\\n",
            "0    Lionel Messi is the only player in history to ...   \n",
            "1    The Great Wall of China cannot be seen from sp...   \n",
            "2    My Wi-Fi always decides to stop working the mo...   \n",
            "3    Michael Phelps has won more Olympic gold medal...   \n",
            "4    Every time I go to the grocery store, I forget...   \n",
            "..                                                 ...   \n",
            "245  Beethoven composed some of his greatest works ...   \n",
            "246  The world’s most expensive coffee comes from b...   \n",
            "247  The human nose can remember over 50,000 differ...   \n",
            "248  The International Space Station orbits Earth a...   \n",
            "249  The first video ever uploaded to YouTube was a...   \n",
            "\n",
            "                                           Telugu_Text  \\\n",
            "0    లియోనెల్ మెస్సీ ఎనిమిది బాలోన్ డి'ఓర్ అవార్డుల...   \n",
            "1    చైనా గ్రేట్ వాల్ ను అంతరిక్షం నుండి నగ్న కళ్ళత...   \n",
            "2    నాకు ఏదైనా ముఖ్యమైన పని ఉన్నప్పుడు నా వై-ఫై ఎల...   \n",
            "3    మైఖేల్ ఫెల్ప్స్ ఒలింపిక్ క్రీడల చరిత్రలో చాలా ...   \n",
            "4    నేను కిరాణా దుకాణానికి వెళ్లిన ప్రతిసారీ, నాకు...   \n",
            "..                                                 ...   \n",
            "245  బీథోవెన్ తన గొప్ప రచనలలో కొన్నింటిని పూర్తిగా ...   \n",
            "246  ప్రపంచంలో అత్యంత ఖరీదైన కాఫీ, సివెట్ పిల్లి జీ...   \n",
            "247  మానవ ముక్కు 50,000 విభిన్న వాసనలను గుర్తుంచుకో...   \n",
            "248  ఇంటర్నేషనల్ స్పేస్ స్టేషన్ గంటకు 28,000 కిలోమీ...   \n",
            "249  యూట్యూబ్కి అప్లోడ్ చేయబడిన మొదటి వీడియో ఒక జూల...   \n",
            "\n",
            "                               Normalized_ENGLISH_Text  \\\n",
            "0    lionel messi is the only player in history to ...   \n",
            "1    the great wall of china cannot be seen from sp...   \n",
            "2    my wifi always decides to stop working the mom...   \n",
            "3    michael phelps has won more olympic gold medal...   \n",
            "4    every time i go to the grocery store i forget ...   \n",
            "..                                                 ...   \n",
            "245  beethoven composed some of his greatest works ...   \n",
            "246  the worlds most expensive coffee comes from be...   \n",
            "247  the human nose can remember over 50000 differe...   \n",
            "248  the international space station orbits earth a...   \n",
            "249  the first video ever uploaded to youtube was a...   \n",
            "\n",
            "                                Normalized_TELUGU_Text  \n",
            "0    లయనల మసస ఎనమద బలన డఓర అవరడల గలచకనన ఏకక కరడకరడ ...  \n",
            "1    చన గరట వల న అతరకష నడ నగన కళళత చడలమ అనద పరజల నమ...  \n",
            "2      నక ఏదన మఖయమన పన ఉననపపడ న వఫ ఎలలపపడ పన చయడ మనసతద  \n",
            "3    మఖల ఫలపస ఒలపక కరడల చరతరల చల దశల కట ఎకకవ బగర పత...  \n",
            "4    నన కరణ దకణనక వళలన పరతసర నక నజగ అవసరమన ఒకక వసతవ...  \n",
            "..                                                 ...  \n",
            "245  బథవన తన గపప రచనలల కననటన పరతగ చవటతనత సగత రసడ పర...  \n",
            "246  పరపచల అతయత ఖరదన కఫ సవట పలల జరణచకనన మరయ వసరజచన ...  \n",
            "247  మనవ మకక 50000 వభనన వసనలన గరతచకగలద అదక మర ఇట వస...  \n",
            "248  ఇటరనషనల సపస సటషన గటక 28000 కలమటరల వగత భమన చటటమ...  \n",
            "249  యటయబక అపలడ చయబడన మదట వడయ ఒక జల ఉనన ఏనగ కలప ఇపప...  \n",
            "\n",
            "[250 rows x 4 columns]\n"
          ]
        }
      ],
      "source": [
        "\n",
        "df['Normalized_ENGLISH_Text'] = df['Normalized_ENGLISH_Text'].apply(lambda x: \" \".join(x.lower()for x in x.split()))\n",
        "print(\"Normalized_ENGLISH_Text after lowercasing english text\",\"\\n\")\n",
        "print(df['Normalized_ENGLISH_Text'])\n",
        "print(\"\\n\")\n",
        "\n",
        "df['Normalized_ENGLISH_Text'] = df['Normalized_ENGLISH_Text']. str.replace(r'[^\\w\\s]', '', regex=True)\n",
        "df['Normalized_TELUGU_Text'] = df['Normalized_TELUGU_Text']. str.replace(r'[^\\w\\s]', '', regex=True)\n",
        "print(\"Normalized_ENGLISH_Text and Normalized_Telugu_Text after removing punctuations \",\"\\n\")\n",
        "print(df)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bC5ROd3TfrbB",
        "outputId": "3f74abf2-b032-44b6-d1d0-21f0611f7ee6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " SentencePiece tokenization completed and saved as 'sentencepiece_tokenized_dataset.xlsx'\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import sentencepiece as spm\n",
        "\n",
        "# 1. Load your Excel dataset\n",
        "df = pd.read_excel(\"/content/Normalized_Dataset.xlsx\")\n",
        "\n",
        "# 2. Use Cleaned raw sentences (not tokenized lists)\n",
        "english_sentences = df[\"Normalized_ENGLISH_Text\"].dropna().astype(str)\n",
        "telugu_sentences = df[\"Normalized_TELUGU_Text\"].dropna().astype(str)\n",
        "\n",
        "english_text_path = \"english_raw.txt\"\n",
        "telugu_text_path = \"telugu_raw.txt\"\n",
        "english_sentences.to_csv(english_text_path, index=False, header=False, sep=\"\\n\", encoding='utf-8') # Removed line_terminator\n",
        "telugu_sentences.to_csv(telugu_text_path, index=False, header=False, sep=\"\\n\", encoding='utf-8') # Removed line_terminator\n",
        "def get_vocab_limit(text_file):\n",
        "\n",
        "    with open(text_file, \"r\", encoding=\"utf-8\") as f:\n",
        "        text = f.read()\n",
        "    unique_chars = set(text)\n",
        "    return len(unique_chars) * 300  # multiplier can vary (300~1000)\n",
        "\n",
        "eng_vocab_size = min(get_vocab_limit(english_text_path), 6849)\n",
        "tel_vocab_size = min(get_vocab_limit(telugu_text_path), 8000)\n",
        "\n",
        "spm.SentencePieceTrainer.train(\n",
        "    input=english_text_path,\n",
        "    model_prefix=\"spm_english\",\n",
        "    vocab_size=eng_vocab_size,\n",
        "    model_type=\"bpe\"\n",
        ")\n",
        "\n",
        "spm.SentencePieceTrainer.train(\n",
        "    input=telugu_text_path,\n",
        "    model_prefix=\"spm_telugu\",\n",
        "    vocab_size=tel_vocab_size,\n",
        "    model_type=\"bpe\"\n",
        ")\n",
        "\n",
        "sp_en = spm.SentencePieceProcessor(model_file=\"spm_english.model\")\n",
        "sp_te = spm.SentencePieceProcessor(model_file=\"spm_telugu.model\")\n",
        "\n",
        "df[\"English_SPM_Tokens\"] = english_sentences.apply(lambda x: sp_en.encode(x, out_type=str))\n",
        "df[\"Telugu_SPM_Tokens\"] = telugu_sentences.apply(lambda x: sp_te.encode(x, out_type=str))\n",
        "\n",
        "df.to_excel(\"sentencepiece_tokenized_dataset.xlsx\", index=False)\n",
        "\n",
        "print(\" SentencePiece tokenization completed and saved as 'sentencepiece_tokenized_dataset.xlsx'\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UmLa_SvvmkO4",
        "outputId": "17803426-ff78-49ed-c377-7b4edba2667f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Vocabulary saved to: combined_english_telugu_vocab.xlsx\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import ast\n",
        "from collections import OrderedDict\n",
        "\n",
        "# Load the tokenized Excel file - CHANGED TO 'sentencepiece_tokenized_dataset.xlsx'\n",
        "dataset_path = \"/content/sentencepiece_tokenized_dataset.xlsx\"  # Update path if needed\n",
        "df = pd.read_excel(dataset_path)\n",
        "\n",
        "# Convert string representations of lists to actual lists\n",
        "df[\"English_Tokens\"] = df[\"English_SPM_Tokens\"].dropna().apply(ast.literal_eval)\n",
        "df[\"Telugu_Tokens\"] = df[\"Telugu_SPM_Tokens\"].dropna().apply(ast.literal_eval)\n",
        "\n",
        "# Flatten tokens while preserving order and uniqueness\n",
        "def build_vocab(token_lists):\n",
        "    seen = set()\n",
        "    vocab = OrderedDict()\n",
        "    idx = 1\n",
        "    for tokens in token_lists:\n",
        "        for token in tokens:\n",
        "            if token not in seen:\n",
        "                seen.add(token)\n",
        "                vocab[token] = idx\n",
        "                idx += 1\n",
        "    return vocab\n",
        "\n",
        "# Create vocabularies\n",
        "english_vocab = build_vocab(df[\"English_Tokens\"])\n",
        "telugu_vocab = build_vocab(df[\"Telugu_Tokens\"])\n",
        "\n",
        "# Convert to DataFrames\n",
        "english_df = pd.DataFrame(list(english_vocab.items()), columns=[\"English_Token\", \"English_ID\"])\n",
        "telugu_df = pd.DataFrame(list(telugu_vocab.items()), columns=[\"Telugu_Token\", \"Telugu_ID\"])\n",
        "\n",
        "# Pad shorter one to match the longer\n",
        "max_len = max(len(english_df), len(telugu_df))\n",
        "english_df = english_df.reindex(range(max_len)).reset_index(drop=True)\n",
        "telugu_df = telugu_df.reindex(range(max_len)).reset_index(drop=True)\n",
        "\n",
        "# Combine side-by-side\n",
        "combined_df = pd.concat([english_df, telugu_df], axis=1)\n",
        "\n",
        "# Save to Excel in a single sheet\n",
        "vocab_excel_path = \"combined_english_telugu_vocab.xlsx\"\n",
        "combined_df.to_excel(vocab_excel_path, index=False)\n",
        "\n",
        "print(\" Vocabulary saved to:\", vocab_excel_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ncFTP00uOuFk",
        "outputId": "39ab634f-8841-4534-e52b-abf63b0f6f7f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Converted tokenized sentences to integer sequences and saved to 'integer_mapped_dataset.xlsx'\n"
          ]
        }
      ],
      "source": [
        "#mapping the tokens to intezer sequences\n",
        "import pandas as pd\n",
        "import ast\n",
        "\n",
        "df = pd.read_excel(\"/content/sentencepiece_tokenized_dataset.xlsx\")\n",
        "\n",
        "df[\"English_Tokens\"] = df[\"English_SPM_Tokens\"].dropna().apply(ast.literal_eval)\n",
        "df[\"Telugu_Tokens\"] = df[\"Telugu_SPM_Tokens\"].dropna().apply(ast.literal_eval)\n",
        "\n",
        "vocab_df = pd.read_excel(\"/content/combined_english_telugu_vocab.xlsx\") # Load the vocabulary Excel file. No need for sheet_name=None\n",
        "english_vocab_df = vocab_df[['English_Token', 'English_ID']] # Access English vocabulary columns\n",
        "telugu_vocab_df = vocab_df[['Telugu_Token', 'Telugu_ID']]  # Access Telugu vocabulary columns\n",
        "\n",
        "\n",
        "english_vocab = dict(zip(english_vocab_df[\"English_Token\"], english_vocab_df[\"English_ID\"]))\n",
        "telugu_vocab = dict(zip(telugu_vocab_df[\"Telugu_Token\"], telugu_vocab_df[\"Telugu_ID\"]))\n",
        "\n",
        "english_vocab.setdefault('<sos>', max(english_vocab.values()) + 1)\n",
        "english_vocab.setdefault('<eos>', max(english_vocab.values()) + 2)\n",
        "\n",
        "telugu_vocab.setdefault('<sos>', max(telugu_vocab.values()) + 1)\n",
        "telugu_vocab.setdefault('<eos>', max(telugu_vocab.values()) + 2)\n",
        "\n",
        "def tokens_to_ids(tokens, vocab, sos=False, eos=False):\n",
        "    ids = [vocab.get(token, vocab.get('<pad>', 0)) for token in tokens]\n",
        "    if sos:\n",
        "        ids = [vocab['<sos>']] + ids\n",
        "    if eos:\n",
        "        ids = ids + [vocab['<eos>']]\n",
        "    return ids\n",
        "\n",
        "df[\"English_IDs\"] = df[\"English_Tokens\"].apply(lambda x: tokens_to_ids(x, english_vocab, sos=True, eos=True))\n",
        "df[\"Telugu_IDs\"] = df[\"Telugu_Tokens\"].apply(lambda x: tokens_to_ids(x, telugu_vocab, sos=True, eos=True))\n",
        "\n",
        "df.to_excel(\"integer_mapped_dataset.xlsx\", index=False)\n",
        "\n",
        "print(\" Converted tokenized sentences to integer sequences and saved to 'integer_mapped_dataset.xlsx'\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ZIo0NuLRUkz",
        "outputId": "7ab59f6d-8689-432d-b905-944f516f0b5a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Padded sequences saved to 'padded_integer_sequences.xlsx'\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import ast\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Load dataset\n",
        "df = pd.read_excel(\"/content/integer_mapped_dataset.xlsx\")\n",
        "\n",
        "df[\"English_Int_Seq\"] = df[\"English_IDs\"].apply(ast.literal_eval)\n",
        "df[\"Telugu_Int_Seq\"] = df[\"Telugu_IDs\"].apply(ast.literal_eval)\n",
        "\n",
        "max_len = 30\n",
        "\n",
        "padded_eng = pad_sequences(df[\"English_Int_Seq\"], maxlen=max_len, padding='post', truncating='post')\n",
        "padded_tel = pad_sequences(df[\"Telugu_Int_Seq\"], maxlen=max_len, padding='post', truncating='post')\n",
        "\n",
        "df[\"Padded_English\"] = padded_eng.tolist()\n",
        "df[\"Padded_Telugu\"] = padded_tel.tolist()\n",
        "\n",
        "df.to_excel(\"padded_integer_sequences.xlsx\", index=False)\n",
        "\n",
        "print(\" Padded sequences saved to 'padded_integer_sequences.xlsx'\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GqeIHcUc0IcT",
        "outputId": "1d8d80dd-d373-4bc0-b9f5-9444a40a664c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "English Tensor Shape: torch.Size([250, 30])\n",
            "Telugu Tensor Shape: torch.Size([250, 30])\n",
            " English Tensors:\n",
            " tensor([[1764,    1,    2,  ..., 1766,    0,    0],\n",
            "        [1764,   27,   28,  ...,    0,    0,    0],\n",
            "        [1764,   43,   44,  ...,    0,    0,    0],\n",
            "        ...,\n",
            "        [1764,   27,  537,  ...,    0,    0,    0],\n",
            "        [1764,   27, 1490,  ...,    0,    0,    0],\n",
            "        [1764,   27,  258,  ..., 1763,  134,  297]])\n",
            "\n",
            " Telugu Tensors:\n",
            " tensor([[2125,    1,    2,  ...,    0,    0,    0],\n",
            "        [2125,   21,   22,  ...,    0,    0,    0],\n",
            "        [2125,   34,   35,  ...,    0,    0,    0],\n",
            "        ...,\n",
            "        [2125,  532,  533,  ...,    0,    0,    0],\n",
            "        [2125, 2106, 1173,  ...,    0,    0,    0],\n",
            "        [2125, 2114, 2115,  ...,    0,    0,    0]])\n"
          ]
        }
      ],
      "source": [
        "#converting into tensors\n",
        "import pandas as pd\n",
        "import torch\n",
        "from ast import literal_eval\n",
        "\n",
        "# Step 1: Load the dataset\n",
        "df = pd.read_excel(\"/content/padded_integer_sequences.xlsx\") # Changed the file name\n",
        "\n",
        "# Step 2: Convert string lists to actual lists\n",
        "english_seqs = df[\"Padded_English\"].dropna().apply(literal_eval).tolist()\n",
        "telugu_seqs = df[\"Padded_Telugu\"].dropna().apply(literal_eval).tolist()\n",
        "\n",
        "# Step 3: Convert lists to PyTorch tensors\n",
        "english_tensors = torch.tensor(english_seqs, dtype=torch.long)\n",
        "telugu_tensors = torch.tensor(telugu_seqs, dtype=torch.long)\n",
        "\n",
        "# Step 4: Check tensor shapes\n",
        "print(\"English Tensor Shape:\", english_tensors.shape)\n",
        "print(\"Telugu Tensor Shape:\", telugu_tensors.shape)\n",
        "#print tensors\n",
        "print(\" English Tensors:\\n\", english_tensors)\n",
        "print(\"\\n Telugu Tensors:\\n\", telugu_tensors)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JixF7Ye83cvZ",
        "outputId": "30db912c-59da-4199-9934-f624d9a13cbd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " English: <sos> ▁Romans ▁used ▁urine ▁cleaning ▁teaching ▁medalist ▁agent ▁ammonia ▁rotates ▁China ▁year ▁content ▁acted ▁solidifying ▁will ▁bleach ▁teaching ▁medalist ▁sensors ▁binge ▁The <unk>\n",
            " Target Telugu: <unk> ▁they ▁Heath ▁Ledger ▁journal ▁Joker ▁fully ▁embody ▁role ▁powerful ▁sock ▁laundry ▁d ▁grocery ▁trillion ▁vanishing ▁detective ▁dimension ▁among <unk>\n",
            "Predicted Telugu:   ▁rhythm  ▁drama ▁again ▁inspired ▁inspired ▁inspired ▁inspired ▁inspired ▁inspired ▁rhythm  ▁probably ▁satisfying ▁unexpected  ▁during ▁again ▁inspired ▁Mundi  ▁hope ▁motivation  ▁DNA  ▁68 ▁lightning  ▁inspired ▁Why ▁tides  ▁dryness ▁should ▁dryness ▁should ▁dryness ▁necessarily   ▁cringy ▁stepping  ▁Mundi ▁Mundi ▁Mundi ▁Mundi\n"
          ]
        }
      ],
      "source": [
        "# batching and suffling dataset using dataloader\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "class TranslationDataset(Dataset):\n",
        "    def __init__(self, input_tensor, target_tensor):\n",
        "        self.input_tensor = input_tensor\n",
        "        self.target_tensor = target_tensor\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.input_tensor)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.input_tensor[idx], self.target_tensor[idx]\n",
        "\n",
        "dataset = TranslationDataset(english_tensors, telugu_tensors) # Changed input_tensor to english_tensors and target_tensor to telugu_tensors\n",
        "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
        "\n",
        "\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import math\n",
        "import torch.nn.functional as F\n",
        "\n",
        "  # Positional Encoding\n",
        "  class PositionalEncoding(nn.Module):\n",
        "      def __init__(self, d_model, max_len=500):\n",
        "          super(PositionalEncoding, self).__init__()\n",
        "          self.encoding = torch.zeros(max_len, d_model)\n",
        "          position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "          div_term = torch.exp(torch.arange(0, d_model, 2).float() *\n",
        "                              -(math.log(10000.0) / d_model))\n",
        "          self.encoding[:, 0::2] = torch.sin(position * div_term)\n",
        "          self.encoding[:, 1::2] = torch.cos(position * div_term)\n",
        "          self.encoding = self.encoding.unsqueeze(0)  # (1, max_len, d_model)\n",
        "\n",
        "      def forward(self, x):\n",
        "          return x + self.encoding[:, :x.size(1)].to(x.device)\n",
        "\n",
        "\n",
        "  # Multi-Head Attention\n",
        "  class MultiHeadAttention(nn.Module):\n",
        "      def __init__(self, d_model, num_heads):\n",
        "          super(MultiHeadAttention, self).__init__()\n",
        "          self.num_heads = num_heads\n",
        "          self.d_model = d_model\n",
        "          self.d_k = d_model // num_heads\n",
        "          self.d_v = d_model // num_heads\n",
        "\n",
        "          self.query = nn.Linear(d_model, d_model)\n",
        "          self.key = nn.Linear(d_model, d_model)\n",
        "          self.value = nn.Linear(d_model, d_model)\n",
        "          self.fc = nn.Linear(d_model, d_model)\n",
        "\n",
        "      def forward(self, x, mask=None):\n",
        "          batch_size, seq_len, _ = x.size()\n",
        "\n",
        "        # Linear projections and reshape for multi-head attention\n",
        "        q = self.query(x).view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)\n",
        "        k = self.key(x).view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)\n",
        "        v = self.value(x).view(batch_size, seq_len, self.num_heads, self.d_v).transpose(1, 2)\n",
        "\n",
        "        # Scaled dot-product attention\n",
        "        attn_scores = torch.matmul(q, k.transpose(-2, -1)) / torch.sqrt(torch.tensor(self.d_k, dtype=torch.float32))\n",
        "\n",
        "        if mask is not None:\n",
        "            attn_scores = attn_scores.masked_fill(mask == 0, float('-inf'))\n",
        "\n",
        "        attn_weights = F.softmax(attn_scores, dim=-1)\n",
        "        attention_output = torch.matmul(attn_weights, v)\n",
        "\n",
        "        # Concatenate heads and pass through final linear layer\n",
        "        attention_output = attention_output.transpose(1, 2).contiguous().view(batch_size, seq_len, self.d_model)\n",
        "        output = self.fc(attention_output)\n",
        "\n",
        "        return output\n",
        "\n",
        "\n",
        "# Feed Forward Network\n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, d_model, d_ff=512):\n",
        "        super(FeedForward, self).__init__()\n",
        "        self.fc1 = nn.Linear(d_model, d_ff)\n",
        "        self.fc2 = nn.Linear(d_ff, d_model)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.fc2(F.relu(self.fc1(x)))\n",
        "\n",
        "\n",
        "# Encoder Layer\n",
        "class EncoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, num_heads, d_ff=512):\n",
        "        super(EncoderLayer, self).__init__()\n",
        "        self.multihead_attn = MultiHeadAttention(d_model, num_heads)\n",
        "        self.feedforward = FeedForward(d_model, d_ff)\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        # Multi-head attention + Add & Norm\n",
        "        attn_output = self.multihead_attn(x, mask)\n",
        "        x = self.norm1(x + attn_output)\n",
        "\n",
        "        # Feedforward + Add & Norm\n",
        "        ff_output = self.feedforward(x)\n",
        "        x = self.norm2(x + ff_output)\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "# Decoder Layer\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class DecoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, num_heads, d_ff=512):\n",
        "        super(DecoderLayer, self).__init__()\n",
        "        self.multihead_attn1 = MultiHeadAttention(d_model, num_heads)  # Self-attention\n",
        "        self.multihead_attn2 = MultiHeadAttention(d_model, num_heads)  # Cross-attention\n",
        "        self.feedforward = FeedForward(d_model, d_ff)\n",
        "\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "        self.norm3 = nn.LayerNorm(d_model)\n",
        "\n",
        "    def forward(self, x, encoder_output, tgt_mask=None, src_mask=None):\n",
        "        # Self-attention (decoder)\n",
        "        attn_output1 = self.multihead_attn1(x, tgt_mask)\n",
        "        x = self.norm1(x + attn_output1)\n",
        "\n",
        "        # Cross-attention (decoder attends to encoder output)\n",
        "        attn_output2 = self.multihead_attn2(x + attn_output1, src_mask)\n",
        "        x = self.norm2(x + attn_output2)\n",
        "\n",
        "        # Feed-forward network\n",
        "        ff_output = self.feedforward(x)\n",
        "        x = self.norm3(x + ff_output)\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "# Full Transformer\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class Transformer(nn.Module):\n",
        "    def __init__(self, vocab_size, d_model, num_heads, num_encoder_layers, num_decoder_layers, d_ff=512, max_len=500):\n",
        "        super(Transformer, self).__init__()\n",
        "\n",
        "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
        "        self.pos_encoder = PositionalEncoding(d_model, max_len)\n",
        "\n",
        "        self.encoder_layers = nn.ModuleList([\n",
        "            EncoderLayer(d_model, num_heads, d_ff) for _ in range(num_encoder_layers)])\n",
        "\n",
        "\n",
        "        self.decoder_layers = nn.ModuleList([\n",
        "            DecoderLayer(d_model, num_heads, d_ff) for _ in range(num_decoder_layers)])\n",
        "\n",
        "\n",
        "        self.fc_out = nn.Linear(d_model, vocab_size)\n",
        "\n",
        "    def forward(self, src, tgt, tgt_mask=None, src_mask=None):\n",
        "        # Embed and add positional encoding\n",
        "        src = self.pos_encoder(self.embedding(src))\n",
        "        tgt = self.pos_encoder(self.embedding(tgt))\n",
        "\n",
        "        # Encoder\n",
        "        encoder_output = src\n",
        "        for layer in self.encoder_layers:\n",
        "            encoder_output = layer(encoder_output, mask=src_mask)\n",
        "\n",
        "        # Decoder\n",
        "        decoder_output = tgt\n",
        "        for layer in self.decoder_layers:\n",
        "            decoder_output = layer(decoder_output, encoder_output, tgt_mask=tgt_mask, src_mask=src_mask)\n",
        "\n",
        "        # Final linear layer to project to vocab size\n",
        "        output = self.fc_out(decoder_output)\n",
        "        return output\n",
        "\n",
        "#initialize the model parameters\n",
        "\n",
        "vocab_size = 3000  # Adjust to your vocabulary size\n",
        "d_model = 256\n",
        "num_heads = 8\n",
        "num_encoder_layers = 4\n",
        "num_decoder_layers = 4\n",
        "max_len = 100\n",
        "\n",
        "model = Transformer(\n",
        "    vocab_size=vocab_size,\n",
        "    d_model=d_model,\n",
        "    num_heads=num_heads,\n",
        "    num_encoder_layers=num_encoder_layers,\n",
        "    num_decoder_layers=num_decoder_layers,\n",
        "    d_ff=512,\n",
        "    max_len=max_len\n",
        ")\n",
        "\n",
        "\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=0)  # ignore PAD\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
        "\n",
        "\n",
        "#train the model\n",
        "def train(model, dataloader, optimizer, criterion, epochs=10):\n",
        "    model.train()\n",
        "    for epoch in range(epochs):\n",
        "        total_loss = 0\n",
        "        for src, tgt in dataloader:\n",
        "            tgt_input = tgt[:, :-1]\n",
        "            tgt_output = tgt[:, 1:]\n",
        "\n",
        "            output = model(src, tgt_input)\n",
        "            output = output.reshape(-1, output.shape[-1])\n",
        "            tgt_output = tgt_output.reshape(-1)\n",
        "\n",
        "            loss = criterion(output, tgt_output)\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        print(f\"Epoch {epoch+1}, Loss: {total_loss:.4f}\")\n",
        "\n",
        "\n",
        "#evaluation usin meteor score\n",
        "from nltk.translate.meteor_score import single_meteor_score\n",
        "\n",
        "def evaluate(model, dataloader, idx_to_word):\n",
        "    model.eval()\n",
        "    meteor_scores = []\n",
        "    with torch.no_grad():\n",
        "        for src, tgt in dataloader:\n",
        "            tgt_input = tgt[:, :-1]\n",
        "            outputs = model(src, tgt_input)\n",
        "            predicted_ids = outputs.argmax(dim=-1)\n",
        "\n",
        "            for pred_seq, true_seq in zip(predicted_ids, tgt[:, 1:]):\n",
        "                pred_sentence = \" \".join([idx_to_word.get(int(idx), \"\") for idx in pred_seq if idx != 0])\n",
        "                true_sentence = \" \".join([idx_to_word.get(int(idx), \"\") for idx in true_seq if idx != 0])\n",
        "                score = single_meteor_score(true_sentence, pred_sentence)\n",
        "                meteor_scores.append(score)\n",
        "\n",
        "    avg_score = sum(meteor_scores) / len(meteor_scores)\n",
        "    print(f\"Average METEOR Score: {avg_score:.4f}\")\n",
        "\n",
        "#translation model\n",
        "def translate_sentence(sentence, model, word_to_idx, idx_to_word, max_len=50):\n",
        "    model.eval()\n",
        "    src_indices = [word_to_idx.get(word, word_to_idx[\"<unk>\"]) for word in sentence.lower().split()]\n",
        "    src_tensor = torch.tensor(src_indices).unsqueeze(0)\n",
        "\n",
        "    tgt_indices = [word_to_idx[\"<sos>\"]]\n",
        "    for _ in range(max_len):\n",
        "        tgt_tensor = torch.tensor(tgt_indices).unsqueeze(0)\n",
        "        with torch.no_grad():\n",
        "            output = model(src_tensor, tgt_tensor)\n",
        "            next_token = output[0, -1].argmax().item()\n",
        "        if next_token == word_to_idx[\"<eos>\"]:\n",
        "            break\n",
        "        tgt_indices.append(next_token)\n",
        "\n",
        "    translated = [idx_to_word.get(idx, \"\") for idx in tgt_indices[1:]]\n",
        "    return \" \".join(translated)\n",
        "\n",
        "# Pick a random sentence from dataset\n",
        "import random\n",
        "\n",
        "# Assuming 'english_vocab' from previous cells\n",
        "word_to_idx = {token: idx for idx, token in enumerate(english_vocab)}\n",
        "idx_to_word = {idx: token for token, idx in word_to_idx.items()}\n",
        "\n",
        "# Add pad token here with index 0\n",
        "word_to_idx['<pad>'] = 0\n",
        "idx_to_word[0] = '<pad>'\n",
        "\n",
        "word_to_idx['<unk>'] = len(word_to_idx)\n",
        "\n",
        "# Add any keys missing in the idx_to_word dictionary\n",
        "# Ensure values are integers before using them in range\n",
        "for idx in range(max(idx_to_word) + 1, int(max(english_vocab.values())) + 1):\n",
        "    idx_to_word[idx] = \"<unk>\"  # Assign a special token like \"<unk>\" to missing keys\n",
        "\n",
        "index = random.randint(0, len(english_tensors)-1)\n",
        "\n",
        "input_tensor = english_tensors[index]\n",
        "target_tensor = telugu_tensors[index]\n",
        "\n",
        "# Convert tensor to sentence (tokens → words)\n",
        "input_sentence = \" \".join([idx_to_word.get(idx.item(), \"<unk>\") for idx in input_tensor if idx.item() != word_to_idx['<pad>']])\n",
        "\n",
        "# Predict using model\n",
        "predicted_sentence = translate_sentence(input_sentence, model, word_to_idx, idx_to_word)\n",
        "\n",
        "# Convert target_tensor to string for comparison\n",
        "target_sentence = \" \".join([idx_to_word.get(idx.item(), \"<unk>\") for idx in target_tensor if idx.item() != word_to_idx['<pad>']])\n",
        "\n",
        "print(\" English:\", input_sentence)\n",
        "print(\" Target Telugu:\", target_sentence)\n",
        "print(\"Predicted Telugu:\", predicted_sentence)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TcUlVdKvvXsB"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2s7aQ0Um4xYw",
        "outputId": "24df6335-14d1-45b2-b80c-4a1fbea6b909"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/transformer.py:385: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0, Loss: 2.6227\n",
            "Epoch 100, Loss: 0.0350\n",
            "Epoch 200, Loss: 0.0127\n",
            "Epoch 300, Loss: 0.0070\n",
            "Epoch 400, Loss: 0.0040\n",
            "Epoch 500, Loss: 0.0028\n",
            "Epoch 600, Loss: 0.0022\n",
            "Epoch 700, Loss: 0.0018\n",
            "Epoch 800, Loss: 0.0012\n",
            "Epoch 900, Loss: 0.0010\n",
            "\n",
            "Input Sentence: The moment I decide to eat healthy someone magically shows up with pizza\n",
            "Predicted Translation: నేను ఆరోగ్యకరమైన ఆహారం తినాలని నిర్ణయించుకున్న క్షణంలో ఎవరో మాయగా పిజ్జాతో ప్రత్యక్షమవుతారు\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "import random\n",
        "\n",
        "# Set manual seed for reproducibility\n",
        "torch.manual_seed(42)\n",
        "\n",
        "\n",
        "sentence_en = \"The moment I decide to eat healthy someone magically shows up with pizza\"\n",
        "sentence_te = \"నేను ఆరోగ్యకరమైన ఆహారం తినాలని నిర్ణయించుకున్న క్షణంలో ఎవరో మాయగా పిజ్జాతో ప్రత్యక్షమవుతారు\"\n",
        "\n",
        "input_words = sentence_en.strip().split()\n",
        "target_words = [\"<sos>\"] + sentence_te.strip().split() + [\"<eos>\"]\n",
        "\n",
        "# Build vocabularies\n",
        "word2idx_en = {word: i for i, word in enumerate(set(input_words))}\n",
        "word2idx_te = {word: i for i, word in enumerate(set(target_words))}\n",
        "word2idx_te[\"<pad>\"] = len(word2idx_te)\n",
        "\n",
        "idx2word_te = {i: w for w, i in word2idx_te.items()}\n",
        "\n",
        "vocab_size_en = len(word2idx_en)\n",
        "vocab_size_te = len(word2idx_te)\n",
        "pad_idx = word2idx_te[\"<pad>\"]\n",
        "\n",
        "\n",
        "def tokenize(sentence, vocab):\n",
        "    return torch.tensor([vocab[word] for word in sentence.split()], dtype=torch.long)\n",
        "\n",
        "input_tensor = tokenize(sentence_en, word2idx_en).unsqueeze(0)\n",
        "target_tensor = tokenize(\" \".join(target_words), word2idx_te).unsqueeze(0)\n",
        "\n",
        "#  Transformer Model\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, max_len=100):\n",
        "        super().__init__()\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len).unsqueeze(1).float()\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * -(math.log(10000.0) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        self.pe = pe.unsqueeze(0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x + self.pe[:, :x.size(1)]\n",
        "\n",
        "class Transformer(nn.Module):\n",
        "    def __init__(self, vocab_en, vocab_te, d_model=64, nhead=4, num_layers=2):\n",
        "        super().__init__()\n",
        "        self.src_embed = nn.Embedding(vocab_en, d_model)\n",
        "        self.tgt_embed = nn.Embedding(vocab_te, d_model)\n",
        "        self.pos_encoder = PositionalEncoding(d_model)\n",
        "        self.transformer = nn.Transformer(d_model, nhead, num_layers, num_layers, dim_feedforward=128)\n",
        "        self.fc_out = nn.Linear(d_model, vocab_te)\n",
        "\n",
        "    def generate_square_subsequent_mask(self, sz):\n",
        "        return torch.triu(torch.ones(sz, sz) * float('-inf'), diagonal=1)\n",
        "\n",
        "    def forward(self, src, tgt):\n",
        "        src = self.src_embed(src)\n",
        "        tgt = self.tgt_embed(tgt)\n",
        "        src = self.pos_encoder(src)\n",
        "        tgt = self.pos_encoder(tgt)\n",
        "\n",
        "        tgt_mask = self.generate_square_subsequent_mask(tgt.size(1)).to(src.device)\n",
        "        out = self.transformer(src.transpose(0,1), tgt.transpose(0,1), tgt_mask=tgt_mask)\n",
        "        return self.fc_out(out.transpose(0,1))\n",
        "\n",
        "\n",
        "model = Transformer(vocab_size_en, vocab_size_te)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=pad_idx)\n",
        "\n",
        "n_epochs = 1000\n",
        "model.train()\n",
        "for epoch in range(n_epochs):\n",
        "    optimizer.zero_grad()\n",
        "    output = model(input_tensor, target_tensor[:, :-1])\n",
        "    loss = criterion(output.reshape(-1, vocab_size_te), target_tensor[:, 1:].reshape(-1))\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    if epoch % 100 == 0:\n",
        "        print(f\"Epoch {epoch}, Loss: {loss.item():.4f}\")\n",
        "\n",
        "\n",
        "\n",
        "def translate(model, input_tensor, max_len=30):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        src = model.src_embed(input_tensor)\n",
        "        src = model.pos_encoder(src)\n",
        "        memory = model.transformer.encoder(src.transpose(0,1))\n",
        "\n",
        "        tgt_tokens = [word2idx_te[\"<sos>\"]]\n",
        "        for _ in range(max_len):\n",
        "            tgt_tensor = torch.tensor(tgt_tokens).unsqueeze(0)\n",
        "            tgt = model.tgt_embed(tgt_tensor)\n",
        "            tgt = model.pos_encoder(tgt)\n",
        "            tgt_mask = model.generate_square_subsequent_mask(tgt.size(1)).to(tgt.device)\n",
        "\n",
        "            out = model.transformer.decoder(tgt.transpose(0,1), memory, tgt_mask=tgt_mask)\n",
        "            logits = model.fc_out(out.transpose(0,1))\n",
        "            next_token = torch.argmax(logits[0, -1]).item()\n",
        "            if next_token == word2idx_te[\"<eos>\"]:\n",
        "                break\n",
        "            tgt_tokens.append(next_token)\n",
        "\n",
        "        return \" \".join(idx2word_te[idx] for idx in tgt_tokens[1:])\n",
        "# Translate\n",
        "output_translation = translate(model, input_tensor)\n",
        "print(\"\\nInput Sentence:\", sentence_en)\n",
        "print(\"Predicted Translation:\", output_translation)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
